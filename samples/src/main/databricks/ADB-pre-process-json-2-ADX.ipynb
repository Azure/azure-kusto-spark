{"cells":[{"cell_type":"markdown","source":["# Data pre-processing for Azure Data Explorer\n\n<img src=\"https://github.com/Azure/azure-kusto-spark/raw/master/kusto_spark.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>\n\nWe often see customer scenarios where historical data has to be migrated to Azure Data Explorer (ADX). Although ADX has very powerful data-transformation capabilities via [update policies](https://docs.microsoft.com/azure/data-explorer/kusto/management/updatepolicy), sometimes more or less complex data engineering tasks must be done upfront. This happens if the original data structure is too complex or just single data elements being too big, hitting data explorer limits of dynamic columns of 1 MB or maximum ingest file-size of 1 GB for uncompressed data (see also [Comparing ingestion methods and tools](https://docs.microsoft.com/azure/data-explorer/ingest-data-overview#comparing-ingestion-methods-and-tools)) .\n\nLet' s think about an Industrial Internet-of-Things (IIoT) use-case where you get data from several production lines. In the production line several devices read humidity, pressure, etc. The following example shows a scenario where a one-to-many relationship is implemented within an array. With this you might get very large columns (with millions of device readings per production line) that might exceed the limit of 1 MB in Azure Data Explorer for dynamic columns.\nIn this case you need to do some pre-processing.\n\n\nData has already been uploaded to Azure storage. You will start reading the json-data into a data frame:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b698725d-b7ba-467e-b6f4-d20842d7ee10"}}},{"cell_type":"code","source":["inputpath = \"wasbs://synapsework@kustosamplefiles.blob.core.windows.net/*.json\"\n\n# optional, for the output to Azure Storage:\n#outputpath = \"<your-storage-path>\"\n\ndf = spark.read.format(\"json\").load(inputpath)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fb78df71-07fa-4248-8f74-552f46a566af"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["The notebook has a parameter IngestDate, this will be used setting the extentsCreationtime. You can call this notebook from Azure Data Factory for all days you want to load to Azure Data Explorer.\nAlternatively you can make use of a partitioning policy."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ff57a41-be30-42eb-b9be-8d5248e68c4e"}}},{"cell_type":"code","source":["dbutils.widgets.text(\"wIngestDate\", \"2021-08-06T00:00:00.000Z\", \"Ingestion Date\")\nIngestDate = dbutils.widgets.get(\"wIngestDate\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"529ffe79-d2c0-44ff-883c-ed386744e2dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["display (df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8decaf7e-0803-4add-9400-f836c4c86081"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We see that the dataframe has some complex datatypes. The only thing that we want to change here is getting rid of the array, so having the resulting dataset a row for every entry in the measurement array. \n\n*How can we achieve this?*\n\npyspark-sql has some very powerful functions for transformations of complex datatypes. We will make use of the [explode-function](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html). In this case explode (\"measurement\") will give us a resulting dataframe with single rows per array-element. Finally we only have to drop the original measurement-column (it is the original structure):"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f575adf4-e914-4526-b47e-2231c1f1dc01"}}},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\ndf_explode = df.select(\"*\", explode(\"measurement\").alias(\"device\")).drop(\"measurement\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d03d5c4f-1097-4c03-aef9-223793b0cbf2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["With this we alreadyhave done the necessary data transformation with one line of code. Let' s do some final \"prettyfying\". \nAs we are already preprocessing the data and want to get rid of the complex data types we select the struct elements to get a simplified table:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3f8ec4a-1c7a-48ef-a312-ec39708f5bde"}}},{"cell_type":"code","source":["df_all_in_column = df_explode.select (\"header.*\", \"device.header.*\", \"device.*\", \"ProdLineData.*\").drop(\"header\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aa5a9ccd-48e9-4e00-a261-10d114a5a4eb"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display (df_all_in_column)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"32147d0c-290a-4fda-b6d3-1d4b913f8996"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["We are setting the extentsCreationTime to the notebook-parameter *IngestDate*. For other ingestion properties see [here](https://github.com/Azure/azure-kusto-spark/blob/master/samples/src/main/python/pyKusto.py)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b7d1e134-18e8-400d-b7c6-33d50c758b8d"}}},{"cell_type":"code","source":["extentsCreationTime = sc._jvm.org.joda.time.DateTime.parse(IngestDate)\nsp = sc._jvm.com.microsoft.kusto.spark.datasink.SparkIngestionProperties(\n        False, None, None, None, None, extentsCreationTime, None, None)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b930eae0-3e19-486f-afbc-9da117c04de7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Finally, we write the resulting dataframe back to to Azure Data Explorer. Prerequisite doing this is: \n* the target table created in the target database (.create table measurement (ProductionLineId : string, deviceId:string, enqueuedTime:datetime, humidity:real, humidity_unit:string, temperature:real, temperature_unit:string,  pressure:real, pressure_unit:string, reading : dynamic))\n* having created a service principal for the ADX access\n* the service principal (AAD-application) accessing ADX has sufficient permissions (add the ingestor and viewer role)\n* Install the latest Kusto library  from maven see also the [Azure Data Explorer Connector for Apache Spark documentation](https://github.com/Azure/azure-kusto-spark#usage)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f2dd8e51-aa6d-41e1-91d5-10d5d54b90df"}}},{"cell_type":"code","source":["df_all_in_column.write. \\\n  format(\"com.microsoft.kusto.spark.datasource\"). \\\n  option(\"kustoCluster\", \"https://<yourcluster>\"). \\\n  option(\"kustoDatabase\", \"your-database\"). \\\n  option(\"kustoTable\", \"<your-table>\"). \\\n  option(\"sparkIngestionPropertiesJson\", sp.toString()). \\\n  option(\"kustoAadAppId\", \"<app-id>\"). \\\n  option(\"kustoAadAppSecret\",dbutils.secrets.get(scope=\"<scope-name>\",key=\"<service-credential-key-name>\"). \\\n  option(\"kustoAadAuthorityID\", \"<tenant-id>\"). \\\n  mode(\"Append\"). \\\n  save()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85974848-8eb7-40bb-ae89-f3a6d7885b2e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You might also consider writing the data to Azure Storage (this might be also make sense for mor complex tranformation pipelines as an intermediate staging step):"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a805cf0-cc99-4c70-bd40-28ae5aef42d2"}}},{"cell_type":"code","source":["# df_all_in_column.write.mode('overwrite').json(outputpath) "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c14b3c43-6c71-459f-a9ed-7f58dd9120be"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ADB-pre-process-json-2-ADX","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":61800690158143}},"nbformat":4,"nbformat_minor":0}
