{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data pre-processing for Azure Data Explorer\r\n",
        "\r\n",
        "<img src=\"https://github.com/Azure/azure-kusto-spark/raw/master/kusto_spark.png\" style=\"border: 1px solid #aaa; border-radius: 10px 10px 10px 10px; box-shadow: 5px 5px 5px #aaa\"/>\r\n",
        "\r\n",
        "We often see customer scenarios where historical data has to be migrated to Azure Data Explorer (ADX). Although ADX has very powerful data-transformation capabilities via [update policies](https://docs.microsoft.com/azure/data-explorer/kusto/management/updatepolicy), sometimes more or less complex data engineering tasks must be done upfront. This happens if the original data structure is too complex or just single data elements being too big, hitting data explorer limits of dynamic columns of 1 MB or maximum ingest file-size of 1 GB for uncompressed data (see also [Comparing ingestion methods and tools](https://docs.microsoft.com/azure/data-explorer/ingest-data-overview#comparing-ingestion-methods-and-tools)) .\r\n",
        "\r\n",
        "Let' s think about an Industrial Internet-of-Things (IIoT) use-case where you get data from several production lines. In the production line several devices read humidity, pressure, etc. The following example shows a scenario where a one-to-many relationship is implemented within an array. With this you might get very large columns (with millions of device readings per production line) that might exceed the limit of 1 MB in Azure Data Explorer for dynamic columns.\r\n",
        "In this case you need to do some pre-processing.\r\n",
        "\r\n",
        "\r\n",
        "Data has already been uploaded to Azure storage. You will start reading the json-data into a data frame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import *\r\n",
        "\r\n",
        "\r\n",
        "# Azure storage access info, in this case demo data\r\n",
        "blob_account_name = 'kustosamplefiles'      # replace with your blob name\r\n",
        "blob_container_name = 'synapsework'         # replace with your container name\r\n",
        "blob_relative_path =  ''                    # replace with your relative folder path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Allow SPARK to access from Blob remotely\r\n",
        "inputpath = 'wasbs://%s@%s.blob.core.windows.net/%s' % (blob_container_name, blob_account_name, blob_relative_path)\r\n",
        "spark.conf.set('fs.azure.sas.%s.%s.blob.core.windows.net' % (blob_container_name, blob_account_name),'')\r\n",
        "print('Remote blob path: ' + inputpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# Primary storage info, here we also write to the Azure Data Lake Store\r\n",
        "account_name =  'your-accountname' # 'fill in your primary account name'fill in your primary account name\r\n",
        "container_name = 'your-container-name'        # 'fill in your container name'fill in your container name\r\n",
        "relative_path =  'your-path'  # fill in your relative folder path'fill in your relative folder path\r\n",
        "\r\n",
        "outputpath = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path)\r\n",
        "print('Target storage account path: ' + outputpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The notebook has a parameter IngestDate, this will be used setting the extentsCreationtime, alternatively you can make use of a partitioning policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "IngestDate = \"2021-08-06T00:00:00.000Z\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "# read the json file\r\n",
        "\r\n",
        "df = spark.read.format(\"json\").load(inputpath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We will see that the dataframe has some complex datatypes. The examination of the datatypes is showing the measurement column which is an array of structs with the measurement data per devicedId:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Displaying the dataframe will show you the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [],
            "yLabel": "",
            "xLabel": "",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "aggData": "{}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We see that the dataframe has some complex datatypes. The only thing that we want to change here is getting rid of the array, so having the resulting dataset a row for every entry in the measurement array. \r\n",
        "\r\n",
        "*How can we achieve this?*\r\n",
        "\r\n",
        "pyspark-sql has some very powerful functions for transformations of complex datatypes. We will make use of the [explode-function](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.functions.explode.html). In this case explode (\"measurement\") will give us a resulting dataframe with single rows per array-element. Finally we only have to drop the original measurement-column (it is the original structure):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "\r\n",
        "df_explode = df.select(\"*\", explode(\"measurement\").alias(\"device\")).drop(\"measurement\")\r\n",
        "df_explode.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "With this we already have done the necessary data transformation with one line of code. Let' s do some final \"prettyfying\". \r\n",
        "As we are already preprocessing the data and want to get rid of the complex data types we select the struct elements to get a simplified table: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [],
            "values": [],
            "yLabel": "",
            "xLabel": "",
            "aggregation": "COUNT",
            "aggByBackend": false
          },
          "aggData": "{}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display(df_explode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The filtering in a \"real-world-scenario\" should happen on a partitioned column, here we are doing this to demonstrate the explicit setting of the extentsCreationTime configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df_all_in_column = df_explode.select (\"header.*\", \"device.header.*\", \"device.*\", \"ProdLineData.*\").drop(\"header\")\r\n",
        "df_all_in_column = df_all_in_column.filter(df_all_in_column.enqueuedTime[0:10]==IngestDate[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false,
        "diagram": {
          "activateDiagramType": 1,
          "chartConfig": {
            "category": "bar",
            "keys": [
              "deviceId"
            ],
            "values": [
              "ProductionLineId"
            ],
            "yLabel": "ProductionLineId",
            "xLabel": "deviceId",
            "aggregation": "SUM",
            "aggByBackend": false
          },
          "aggData": "{\"ProductionLineId\":{\"0\":106,\"1\":85,\"2\":69,\"3\":86,\"4\":66,\"999\":0}}",
          "isSummary": false,
          "previewData": {
            "filter": null
          },
          "isSql": false
        }
      },
      "source": [
        "display (df_all_in_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "We are setting the extentsCreationTime to the notebook-parameter *IngestDate*. For other ingestion properties see [here](https://github.com/Azure/azure-kusto-spark/blob/master/samples/src/main/python/pyKusto.py)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "extentsCreationTime = sc._jvm.org.joda.time.DateTime.parse(IngestDate)\r\n",
        "sp = sc._jvm.com.microsoft.kusto.spark.datasink.SparkIngestionProperties(\r\n",
        "        False, None, None, None, None, extentsCreationTime, None, None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Finally, we write the resulting dataframe back to to Azure Data Explorer. Prerequisite doing this in Synapse Analytics is \r\n",
        "* having created a linked Service (detailed steps for the setup you can find in the [documentation](https://docs.microsoft.com/azure/synapse-analytics/quickstart-connect-azure-data-explorer))\r\n",
        "* the target table created in the target database (.create table measurement (ProductionLineId : string, deviceId:string, enqueuedTime:datetime, humidity:real, humidity_unit:string, temperature:real, temperature_unit:string,  pressure:real, pressure_unit:string, reading : dynamic))\r\n",
        "* the credential accessing ADX has sufficient permissions (add the ingestor and viewer role)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df_all_in_column.write \\\r\n",
        "    .format(\"com.microsoft.kusto.spark.synapse.datasource\") \\\r\n",
        "    .option(\"spark.synapse.linkedService\", \"<your linked service to ADX\") \\\r\n",
        "    .option(\"kustoDatabase\", \"<your ADX database>\") \\\r\n",
        "    .option(\"kustoTable\", \"your ADX table\") \\\r\n",
        "    .option(\"sparkIngestionPropertiesJson\", sp.toString()) \\\r\n",
        "    .mode(\"Append\") \\\r\n",
        "    .save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "You might also consider writing the data to Azure Storage (this might be also make sense for mor complex tranformation pipelines as an intermediate staging step):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        "df_all_in_column.write.mode('overwrite').json(outputpath) "
      ]
    }
  ]
}